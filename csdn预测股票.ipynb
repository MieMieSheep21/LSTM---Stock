{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f015a63-49e7-463c-a6d1-923f9f0237c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本接口即将停止更新，请尽快使用Pro版接口：https://tushare.pro/document/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\tushare\\stock\\trading.py:706: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append(_get_k_data(url, dataflag,\n",
      "D:\\anaconda\\Lib\\site-packages\\tushare\\stock\\trading.py:706: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append(_get_k_data(url, dataflag,\n",
      "D:\\anaconda\\Lib\\site-packages\\tushare\\stock\\trading.py:706: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append(_get_k_data(url, dataflag,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model_total parameter: 0.00122500M\n",
      "Epoch: 1, Loss:0.31205\n",
      "Epoch: 2, Loss:0.25850\n",
      "Epoch: 3, Loss:0.20651\n",
      "Epoch: 4, Loss:0.15203\n",
      "Epoch: 5, Loss:0.09708\n",
      "Epoch: 6, Loss:0.05114\n",
      "Epoch: 7, Loss:0.03173\n",
      "Epoch: 8, Loss:0.04605\n",
      "Epoch: 9, Loss:0.06506\n",
      "Epoch: 10, Loss:0.06899\n",
      "Epoch: 11, Loss:0.06244\n",
      "Epoch: 12, Loss:0.05256\n",
      "Epoch: 13, Loss:0.04418\n",
      "Epoch: 14, Loss:0.03943\n",
      "Epoch: 15, Loss:0.03822\n",
      "Epoch: 16, Loss:0.03932\n",
      "Epoch: 17, Loss:0.04132\n",
      "Epoch: 18, Loss:0.04314\n",
      "Epoch: 19, Loss:0.04417\n",
      "Epoch: 20, Loss:0.04418\n",
      "Epoch: 21, Loss:0.04321\n",
      "Epoch: 22, Loss:0.04144\n",
      "Epoch: 23, Loss:0.03914\n",
      "Epoch: 24, Loss:0.03660\n",
      "Epoch: 25, Loss:0.03410\n",
      "Epoch: 26, Loss:0.03187\n",
      "Epoch: 27, Loss:0.03001\n",
      "Epoch: 28, Loss:0.02846\n",
      "Epoch: 29, Loss:0.02700\n",
      "Epoch: 30, Loss:0.02526\n",
      "Epoch: 31, Loss:0.02289\n",
      "Epoch: 32, Loss:0.01983\n",
      "Epoch: 33, Loss:0.01647\n",
      "Epoch: 34, Loss:0.01359\n",
      "Epoch: 35, Loss:0.01201\n",
      "Epoch: 36, Loss:0.01178\n",
      "Epoch: 37, Loss:0.01143\n",
      "Epoch: 38, Loss:0.00975\n",
      "Epoch: 39, Loss:0.00727\n",
      "Epoch: 40, Loss:0.00560\n",
      "Epoch: 41, Loss:0.00554\n",
      "Epoch: 42, Loss:0.00602\n",
      "Epoch: 43, Loss:0.00554\n",
      "Epoch: 44, Loss:0.00441\n",
      "Epoch: 45, Loss:0.00414\n",
      "Epoch: 46, Loss:0.00509\n",
      "Epoch: 47, Loss:0.00546\n",
      "Epoch: 48, Loss:0.00470\n",
      "Epoch: 49, Loss:0.00438\n",
      "Epoch: 50, Loss:0.00493\n",
      "Epoch: 51, Loss:0.00506\n",
      "Epoch: 52, Loss:0.00438\n",
      "Epoch: 53, Loss:0.00399\n",
      "Epoch: 54, Loss:0.00431\n",
      "Epoch: 55, Loss:0.00433\n",
      "Epoch: 56, Loss:0.00384\n",
      "Epoch: 57, Loss:0.00374\n",
      "Epoch: 58, Loss:0.00407\n",
      "Epoch: 59, Loss:0.00409\n",
      "Epoch: 60, Loss:0.00381\n",
      "Epoch: 61, Loss:0.00382\n",
      "Epoch: 62, Loss:0.00401\n",
      "Epoch: 63, Loss:0.00390\n",
      "Epoch: 64, Loss:0.00365\n",
      "Epoch: 65, Loss:0.00367\n",
      "Epoch: 66, Loss:0.00374\n",
      "Epoch: 67, Loss:0.00359\n",
      "Epoch: 68, Loss:0.00348\n",
      "Epoch: 69, Loss:0.00357\n",
      "Epoch: 70, Loss:0.00359\n",
      "Epoch: 71, Loss:0.00348\n",
      "Epoch: 72, Loss:0.00348\n",
      "Epoch: 73, Loss:0.00354\n",
      "Epoch: 74, Loss:0.00348\n",
      "Epoch: 75, Loss:0.00340\n",
      "Epoch: 76, Loss:0.00341\n",
      "Epoch: 77, Loss:0.00341\n",
      "Epoch: 78, Loss:0.00333\n",
      "Epoch: 79, Loss:0.00331\n",
      "Epoch: 80, Loss:0.00333\n",
      "Epoch: 81, Loss:0.00329\n",
      "Epoch: 82, Loss:0.00326\n",
      "Epoch: 83, Loss:0.00327\n",
      "Epoch: 84, Loss:0.00326\n",
      "Epoch: 85, Loss:0.00322\n",
      "Epoch: 86, Loss:0.00321\n",
      "Epoch: 87, Loss:0.00320\n",
      "Epoch: 88, Loss:0.00316\n",
      "Epoch: 89, Loss:0.00314\n",
      "Epoch: 90, Loss:0.00314\n",
      "Epoch: 91, Loss:0.00311\n",
      "Epoch: 92, Loss:0.00309\n",
      "Epoch: 93, Loss:0.00309\n",
      "Epoch: 94, Loss:0.00307\n",
      "Epoch: 95, Loss:0.00304\n",
      "Epoch: 96, Loss:0.00303\n",
      "Epoch: 97, Loss:0.00301\n",
      "Epoch: 98, Loss:0.00299\n",
      "Epoch: 99, Loss:0.00298\n",
      "Epoch: 100, Loss:0.00296\n",
      "Epoch: 101, Loss:0.00294\n",
      "Epoch: 102, Loss:0.00293\n",
      "Epoch: 103, Loss:0.00291\n",
      "Epoch: 104, Loss:0.00289\n",
      "Epoch: 105, Loss:0.00288\n",
      "Epoch: 106, Loss:0.00286\n",
      "Epoch: 107, Loss:0.00284\n",
      "Epoch: 108, Loss:0.00283\n",
      "Epoch: 109, Loss:0.00281\n",
      "Epoch: 110, Loss:0.00279\n",
      "Epoch: 111, Loss:0.00278\n",
      "Epoch: 112, Loss:0.00276\n",
      "Epoch: 113, Loss:0.00275\n",
      "Epoch: 114, Loss:0.00273\n",
      "Epoch: 115, Loss:0.00271\n",
      "Epoch: 116, Loss:0.00270\n",
      "Epoch: 117, Loss:0.00268\n",
      "Epoch: 118, Loss:0.00267\n",
      "Epoch: 119, Loss:0.00265\n",
      "Epoch: 120, Loss:0.00264\n",
      "Epoch: 121, Loss:0.00262\n",
      "Epoch: 122, Loss:0.00261\n",
      "Epoch: 123, Loss:0.00259\n",
      "Epoch: 124, Loss:0.00257\n",
      "Epoch: 125, Loss:0.00256\n",
      "Epoch: 126, Loss:0.00254\n",
      "Epoch: 127, Loss:0.00253\n",
      "Epoch: 128, Loss:0.00251\n",
      "Epoch: 129, Loss:0.00250\n",
      "Epoch: 130, Loss:0.00248\n",
      "Epoch: 131, Loss:0.00247\n",
      "Epoch: 132, Loss:0.00245\n",
      "Epoch: 133, Loss:0.00244\n",
      "Epoch: 134, Loss:0.00242\n",
      "Epoch: 135, Loss:0.00241\n",
      "Epoch: 136, Loss:0.00239\n",
      "Epoch: 137, Loss:0.00238\n",
      "Epoch: 138, Loss:0.00236\n",
      "Epoch: 139, Loss:0.00235\n",
      "Epoch: 140, Loss:0.00233\n",
      "Epoch: 141, Loss:0.00232\n",
      "Epoch: 142, Loss:0.00231\n",
      "Epoch: 143, Loss:0.00229\n",
      "Epoch: 144, Loss:0.00228\n",
      "Epoch: 145, Loss:0.00226\n",
      "Epoch: 146, Loss:0.00225\n",
      "Epoch: 147, Loss:0.00224\n",
      "Epoch: 148, Loss:0.00222\n",
      "Epoch: 149, Loss:0.00221\n",
      "Epoch: 150, Loss:0.00219\n",
      "Epoch: 151, Loss:0.00218\n",
      "Epoch: 152, Loss:0.00217\n",
      "Epoch: 153, Loss:0.00215\n",
      "Epoch: 154, Loss:0.00214\n",
      "Epoch: 155, Loss:0.00213\n",
      "Epoch: 156, Loss:0.00212\n",
      "Epoch: 157, Loss:0.00210\n",
      "Epoch: 158, Loss:0.00209\n",
      "Epoch: 159, Loss:0.00208\n",
      "Epoch: 160, Loss:0.00207\n",
      "Epoch: 161, Loss:0.00205\n",
      "Epoch: 162, Loss:0.00204\n",
      "Epoch: 163, Loss:0.00203\n",
      "Epoch: 164, Loss:0.00202\n",
      "Epoch: 165, Loss:0.00201\n",
      "Epoch: 166, Loss:0.00200\n",
      "Epoch: 167, Loss:0.00198\n",
      "Epoch: 168, Loss:0.00197\n",
      "Epoch: 169, Loss:0.00196\n",
      "Epoch: 170, Loss:0.00195\n",
      "Epoch: 171, Loss:0.00194\n",
      "Epoch: 172, Loss:0.00193\n",
      "Epoch: 173, Loss:0.00192\n",
      "Epoch: 174, Loss:0.00191\n",
      "Epoch: 175, Loss:0.00190\n",
      "Epoch: 176, Loss:0.00189\n",
      "Epoch: 177, Loss:0.00188\n",
      "Epoch: 178, Loss:0.00187\n",
      "Epoch: 179, Loss:0.00186\n",
      "Epoch: 180, Loss:0.00185\n",
      "Epoch: 181, Loss:0.00184\n",
      "Epoch: 182, Loss:0.00183\n",
      "Epoch: 183, Loss:0.00182\n",
      "Epoch: 184, Loss:0.00181\n",
      "Epoch: 185, Loss:0.00180\n",
      "Epoch: 186, Loss:0.00179\n",
      "Epoch: 187, Loss:0.00179\n",
      "Epoch: 188, Loss:0.00178\n",
      "Epoch: 189, Loss:0.00177\n",
      "Epoch: 190, Loss:0.00176\n",
      "Epoch: 191, Loss:0.00175\n",
      "Epoch: 192, Loss:0.00174\n",
      "Epoch: 193, Loss:0.00174\n",
      "Epoch: 194, Loss:0.00173\n",
      "Epoch: 195, Loss:0.00172\n",
      "Epoch: 196, Loss:0.00171\n",
      "Epoch: 197, Loss:0.00170\n",
      "Epoch: 198, Loss:0.00170\n",
      "Epoch: 199, Loss:0.00169\n",
      "Epoch: 200, Loss:0.00168\n",
      "The training time took 1.02 mins.\n",
      "The starting time was  Sun Sep 29 00:03:54 2024\n",
      "The finishing time was  Sun Sep 29 00:04:55 2024\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- encoding: utf-8 -*-\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tushare as ts\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "DAYS_FOR_TRAIN = 10\n",
    "\n",
    "\n",
    "class LSTM_Regression(nn.Module):\n",
    "    \"\"\"\n",
    "        使用LSTM进行回归\n",
    "        \n",
    "        参数：\n",
    "        - input_size: feature size\n",
    "        - hidden_size: number of hidden units\n",
    "        - output_size: number of output\n",
    "        - num_layers: layers of LSTM to stack\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, _x):\n",
    "        x, _ = self.lstm(_x)  # _x is input, size (seq_len, batch, input_size)\n",
    "        s, b, h = x.shape  # x is output, size (seq_len, batch, hidden_size)\n",
    "        x = x.view(s*b, h)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(s, b, -1)  # 把形状改回来\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_dataset(data, days_for_train=5) -> (np.array, np.array):\n",
    "    \"\"\"\n",
    "        根据给定的序列data，生成数据集\n",
    "        \n",
    "        数据集分为输入和输出，每一个输入的长度为days_for_train，每一个输出的长度为1。\n",
    "        也就是说用days_for_train天的数据，对应下一天的数据。\n",
    "\n",
    "        若给定序列的长度为d，将输出长度为(d-days_for_train+1)个输入/输出对\n",
    "    \"\"\"\n",
    "    dataset_x, dataset_y= [], []\n",
    "    for i in range(len(data)-days_for_train):\n",
    "        _x = data[i:(i+days_for_train)]\n",
    "        dataset_x.append(_x)\n",
    "        dataset_y.append(data[i+days_for_train])\n",
    "    return (np.array(dataset_x), np.array(dataset_y))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    t0 = time.time()\n",
    "    data_close = ts.get_k_data('000001', start='2019-01-01', index=True)['close']  # 取上证指数的收盘价\n",
    "    data_close.to_csv('000001.csv', index=False) #将下载的数据转存为.csv格式保存\n",
    "    data_close = pd.read_csv('000001.csv') #读取文件\n",
    "    #df_sh = ts.get_k_data('sh', start='2019-01-01', end=datetime.datetime.now().strftime('%Y-%m-%d'))\n",
    "    #print(df_sh.shape)\n",
    "    data_close = data_close.astype('float32').values  # 转换数据类型\n",
    "    plt.plot(data_close)\n",
    "    plt.savefig('data.png', format='png', dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # 将价格标准化到0~1\n",
    "    max_value = np.max(data_close)\n",
    "    min_value = np.min(data_close)\n",
    "    data_close = (data_close - min_value) / (max_value - min_value)\n",
    "\n",
    "    dataset_x, dataset_y = create_dataset(data_close, DAYS_FOR_TRAIN)\n",
    "\n",
    "    # 划分训练集和测试集，70%作为训练集\n",
    "    train_size = int(len(dataset_x) * 0.7)\n",
    "\n",
    "    train_x = dataset_x[:train_size]\n",
    "    train_y = dataset_y[:train_size]\n",
    "\n",
    "    # 将数据改变形状，RNN 读入的数据维度是 (seq_size, batch_size, feature_size)\n",
    "    train_x = train_x.reshape(-1, 1, DAYS_FOR_TRAIN)\n",
    "    train_y = train_y.reshape(-1, 1, 1)\n",
    "\n",
    "    # 转为pytorch的tensor对象\n",
    "    train_x = torch.from_numpy(train_x)\n",
    "    train_y = torch.from_numpy(train_y)\n",
    "\n",
    "\n",
    "    model = LSTM_Regression(DAYS_FOR_TRAIN, 8, output_size=1, num_layers=2) # 导入模型并设置模型的参数输入输出层、隐藏层等\n",
    "\n",
    "\t\n",
    "    model_total = sum([param.nelement() for param in model.parameters()]) # 计算模型参数\n",
    "    print(\"Number of model_total parameter: %.8fM\" % (model_total/1e6))\n",
    "\n",
    "\n",
    "    train_loss = []\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    for i in range(200):\n",
    "        out = model(train_x)\n",
    "        loss = loss_function(out, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.append(loss.item())\n",
    "\t\t\n",
    "\t\t# 将训练过程的损失值写入文档保存，并在终端打印出来\n",
    "        with open('log.txt', 'a+') as f:\n",
    "            f.write('{} - {}\\n'.format(i+1, loss.item()))\n",
    "        if (i+1) % 1 == 0:\n",
    "            print('Epoch: {}, Loss:{:.5f}'.format(i+1, loss.item()))\n",
    "\n",
    "    # 画loss曲线\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss, 'b', label='loss')\n",
    "    plt.title(\"Train_Loss_Curve\")\n",
    "    plt.ylabel('train_loss')\n",
    "    plt.xlabel('epoch_num')\n",
    "    plt.savefig('loss.png', format='png', dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # torch.save(model.state_dict(), 'model_params.pkl')  # 可以保存模型的参数供未来使用\n",
    "    t1=time.time()\n",
    "    T=t1-t0\n",
    "    print('The training time took %.2f'%(T/60)+' mins.')\n",
    "\n",
    "    tt0=time.asctime(time.localtime(t0))\n",
    "    tt1=time.asctime(time.localtime(t1))\n",
    "    print('The starting time was ',tt0)\n",
    "    print('The finishing time was ',tt1)\n",
    "\n",
    "\n",
    "    # for test\n",
    "    model = model.eval() # 转换成测试模式\n",
    "    # model.load_state_dict(torch.load('model_params.pkl'))  # 读取参数\n",
    "\n",
    "    # 注意这里用的是全集 模型的输出长度会比原数据少DAYS_FOR_TRAIN 填充使长度相等再作图\n",
    "    dataset_x = dataset_x.reshape(-1, 1, DAYS_FOR_TRAIN)  # (seq_size, batch_size, feature_size)\n",
    "    dataset_x = torch.from_numpy(dataset_x)\n",
    "\n",
    "    pred_test = model(dataset_x) # 全量训练集\n",
    "    # 的模型输出 (seq_size, batch_size, output_size)\n",
    "    pred_test = pred_test.view(-1).data.numpy()\n",
    "    pred_test = np.concatenate((np.zeros(DAYS_FOR_TRAIN), pred_test))  # 填充0 使长度相同\n",
    "    assert len(pred_test) == len(data_close)\n",
    "\n",
    "    plt.plot(pred_test, 'r', label='prediction')\n",
    "    plt.plot(data_close, 'b', label='real')\n",
    "    plt.plot((train_size, train_size), (0, 1), 'g--')  # 分割线 左边是训练数据 右边是测试数据的输出\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('result.png', format='png', dpi=200)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0845aeb-7354-4295-b28a-58a82a499d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
